# LLM 与 强化学习

## 6.1 强化学习概述

1. 强化学习两大部分——智能体和环境

2. 强化学习的发展历史：马尔可夫决策过程（MDP）——Q-learning——DQN，PPO

3. 强化学习示例：机器狗抓飞盘，智能体：机器狗，环境：飞盘，奖励：抓到飞盘

4. 强化学习基本概念：智能体与环境，序列决策，观测与状态，动作空间，策略，价值函数

5. 强化学习智能体的分类

- 基于策略的智能体
- 基于价值的智能体
- 演员-评论家智能体

区分基于策略和基于价值的智能体：智能体在给定状态下从动作集合中选择一个动作的依据。 在基于策略的强化学习方法中，智能体会制定一套动作策略（确定在给定状态下需要采取何种动作），并根据这个策略进行操作。强化学习算法直接对策略进行优化，使制定的策略能够获得最大的奖励。而在基于价值的强化学习方法中，智能体不需要制定显式的策略，它维护一个价值表格或价值函数，并通过这个价值表格或价值函数来选取价值最大的动作。

不同智能体的强化学习代码示例

gymnasium是一个开源的强化学习库，提供了很多强化学习环境，包括经典的CartPole，MountainCar等。可以方便的使用gymnasium来测试强化学习算法。

简单强化学习示例，Q-learning算法示例，DQN算法示例

[flappybird游戏-Qlearning示例](https://enhuiz.github.io/flappybird-ql/)

Q-learning使用表格来存储Q值，即状态-动作对的价值，Q(s,a)得到reward，Q表格中记录的是未来的总奖励

[深度强化学习之深度Q网络DQN详解](https://zhuanlan.zhihu.com/p/145102068)

DQN中神经网络的输出值实际上为reward，DQN包含训练网络和目标网络
![DQN训练网络和目标网络](https://s2.loli.net/2024/02/03/AEjLmg5bP1h3V9x.png)


[动手学强化学习](https://hrl.boyuai.com/)

DQN算法示例

DQN使用神经网络来拟合Q函数，

6. 强化学习与有监督学习的区别

文献[24]:[Training language models to follow instructions with human feedback](https://arxiv.org/pdf/2203.02155.pdf)
这篇论文主要探讨了如何使大型语言模型更好地遵循用户的意图。尽管增加模型的大小并不一定能提高其性能，大型模型仍可能生成不真实、有害或对用户无帮助的内容。为了解决这个问题，作者提出了一种通过使用人类反馈进行微调的方法来使语言模型与用户意图对齐。
研究人员首先使用由标注者编写的提示和通过OpenAI API提交的提示来收集数据集，这些数据集展示了期望的模型行为。然后，他们使用这些数据对GPT-3进行了监督学习微调。接着，他们收集了一个模型输出排名的数据集，并使用这些数据集通过强化学习进一步微调已经经过监督学习的模型。
微调后的模型被称为InstructGPT。在人类评估中，即使参数数量只有GPT-3的1/100（1.3B相比于175B），InstructGPT模型生成的输出仍然比GPT-3更受欢迎。此外，InstructGPT在提高真实性和减少有害输出方面表现出进步，同时在公共自然语言处理（NLP）数据集上的性能回退很小。
尽管InstructGPT仍然会犯一些简单的错误，结果表明使用人类反馈进行微调是一个有前景的方向，可以使语言模型更好地与人类意图对齐。

## 6.2 奖励模型

1. 数据收集

文献[157]:[Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback](https://arxiv.org/pdf/2204.05862.pdf)
这篇论文主要探讨了如何通过偏好建模和基于人类反馈的强化学习（RLHF）来微调语言模型，使其成为有帮助且无害的助手。研究发现，这种对齐训练几乎在所有自然语言处理（NLP）评估中都提高了性能，并且完全兼容于针对特定技能（如Python编程和摘要生成）的训练。


有用性，无害性。借助Amazon Mechanical Turk收集数据，标准这从模型输出的两个回答中选择一个，记录该回答的数据对，不记录权重

有用性和无害性是相对的。

2. 模型训练

奖励模型架构，通常基于transformer，移除最后一个非嵌入层(通常是自注意力层或前馈网络层)，加上一个线性层，模型输出为标量奖励值。
instructGPT的奖励模型是在GPT-3模型的基础上加一个全连接层，输出一个标量的奖励值。

奖励模型损失函数
文献[158]:[Learning to summarize from human feedback](https://arxiv.org/pdf/2009.01325.pdf)
这篇论文主要探讨了如何通过优化人类偏好来改善生成文献摘要的质量。作者指出，现有的文摘模型通常使用人类参考总结来训练，并使用ROUGE这样的评估指标来评价模型性能，但这些指标并不能很好地反映总结的真实质量。奖励模型的损失函数的定义


模仿学习
文献[159]:[A General Language Assistant as a Laboratory for Alignment](https://arxiv.org/pdf/2112.00861.pdf)
这篇论文主要讨论了基于大型语言模型的通用文本助手的概念，该助手与人类价值观保持一致，即具有帮助性、诚实性和无害性。论文首先探讨了一些简单的基准技术和评估方法，例如提示技术。研究发现，通过适度的干预可以增加模型规模的好处，这种干预方法适用于各种价值观评估，并且不会损害大型模型的性能。
接下来，论文探讨了与对齐性相关的几种训练目标的扩展趋势，比较了模仿学习、二元判别和排名偏好建模。研究发现，排名偏好建模的表现要比模仿学习好得多，而且通常随着模型规模的增大而更有利。相比之下，二元判别通常与模仿学习表现和扩展趋势非常相似。
最后，论文研究了一个名为“偏好模型预训练”的训练阶段，旨在提高在人类偏好微调中的样本效率。

KL散度，相对熵，交叉熵，统计学意义

3. 开源数据集

- OpenAI  Summarize from Feedback数据集
- WebGPT数据集 WebGPT是在GPT3基础上微调的可在网络浏览环境上回答长篇问题的大模型，允许模型搜索和浏览网页。
这篇论文主要介绍了一种方法，通过对 GPT-3 进行微调，使其能够在基于文本的网络浏览环境中回答长篇问题，从而让模型能够搜索和浏览网络。作者通过设置任务使其能够被人类执行，然后利用模仿学习对模型进行训练，并使用人类反馈来优化答案质量。为了让人类更容易对事实准确性进行评估，模型在浏览过程中必须收集支持其答案的参考资料。作者在 Reddit 用户提出的问题数据集 ELI5 上对模型进行了训练和评估。他们通过对 GPT-3 进行行为克隆微调，并使用一个训练有人类偏好预测能力的奖励模型进行拒绝抽样，得到了最佳模型。
- Anthropic HH-RLHF数据集
- Stanford Human Preferences数据集，Reddit帖子和点赞最多的评论，Reddit是一个社交新闻聚合、网上论坛和社交网站，用户可以在网站上发布内容，这些内容被投票排名。

## 6.3 近端策略优化

1. 策略梯度

概念：状态$s_t$，动作$a_t$，策略函数$\pi_\theta$，轨迹$\tau$，轨迹发生概率$p_\theta (\tau)$，累计奖励(回报)$R(\tau)$

累计奖励的推导，计算累计奖励的期望，使用梯度上升法更新策略参数。

策略梯度的推导：求对数的导，复合函数求导，

策略梯度：策略梯度的期望等于累计奖励的期望乘以动作概率的对数梯度

期望难以直接计算，采样N条轨迹估计期望


技巧：
- 概率归一化，添加基线
- 动作奖励权重，该动作的奖励与之前的无关
- 对未来的奖励进行折扣

2. 广义优势估计


优势函数-动作价值函数，状态价值函数

优势函数两种计算方法
- 通过蒙特卡洛采样计算
- 通过时序差分方法计算

广义优势估计：k步优势的指数平均，[推导过程](https://zhuanlan.zhihu.com/p/549145459)


3. PPO算法

- 同策略和异策略
- 重要性采样 重要性采样要求
- 重要性采样融入梯度更新

- PPO算法的两种形式
近端策略优化惩罚和近端策略优化裁剪

## 6.4 MOSS-RLHF

复旦NLP组的一篇论文分析了RLHF框架，评估了PPO算法的内部工作原理，发现策略约束是PPO算法的关键。开源了MOSS-RLHF框架和适合中英文的奖励模型。这篇论文探讨了哪些技巧在PPO算法中是比较关键的，哪些指标能够反映RLHF训练过程中和训练后的模型状态，然后用PPO-Max来表示最适合LLM的实现方式。首先根据历史均值和方差记录对当前奖励组进行归一化和裁剪，然后添加KL-惩罚项来约束策略优化。在模型加载阶段，用奖励模型初始化critic模型，并在正式应用PPO之前对其进行预训练，然后使用全局梯度裁剪并设置较小的经验缓冲区。为了减少对齐tax，在策略优化中添加了预训练语言模型损失，如InstructGPT，并同时对估值函数损失进行裁剪。

参考博客[复旦NLP组开源PPO-Max：32页论文详解RLHF背后秘密，高效对齐人类偏好](https://www.51cto.com/article/761044.html)

PPO温和的引导策略走向优化

## 6.5 总结

在人类偏好数据的标注过程中，人工标注质量良莠不齐，采用奖励模型可以确保一致性，但是奖励模型的底座大小和性能直接关系到评分的泛化能力，建议采取较大的底座模型。

PPO-Max算法，确保RLHF的稳定运行。PPO训练经常出现"Reward Hacking"现象，“Reward Hacking”现象一般是在奖励函数设置不当的情况下，智能体只关注于累计奖励，而偏离了预先的目标。增加模型输出和SFT模型的KL惩罚可以避免这一问题。

[Reward Hacking](https://openai.com/research/faulty-reward-functions)

PPO训练后模型的评估，人工评估成本较高，使用GPT-4评估需要精心设置提示语

本节介绍了强化学习的基本概念，奖励模型的训练流程，PPO算法的内部工作原理等，并以MOSS-RLHF为例，介绍了大语言模型中RLHF的实现方式。


